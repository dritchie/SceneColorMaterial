\section{Probabilistic Model}
\label{sec:model}

In every subsection, motivate why this particular component is important/difficult and why we had to develop something new. Briefly relate to algorithmic details from relevant previous work.

%To be safe, temporarily separating out the spatial arrangement and color compatibility subsection
\input{spatialcolor.tex}

\subsection{Sampling}
\label{sec:sampling}

We use the Metropolis-Hastings algorithm (MH), a variant of Markov Chain Monte Carlo (MCMC), to sample coloring suggestions from the model~\cite{Metropolis,Hastings}. MH explores the coloring state space by \emph{proposing} candidate new states; these proposals are accepted with probability proportional to their model score. Our sampler uses the following proposals:
\begin{itemize}
	\item{\textbf{Perturb} a randomly chosen color by $v \sim \mathcal{N}(0, \sigma)$ in RGB color space}
	\item{\textbf{Swap} two randomly chosen colors}
\end{itemize}
where $\sigma$ varies linearly with the model temperature $\tau$~\remark{Introduce this in an earlier section}. The sampler chooses between these two proposals with probability $\rho$, which also varies linearly with temperature. Since the RGB color space is bounded, the perturbation proposal draws from a truncated normal distribution in order to maintain ergodicity of the chain~\cite{TruncatedGaussians}.

Asymptotically, MCMC samples states with a frequency proportional to their probability under the model. In practice, it can take a prohibitively long time to explore all the modes of a distribution as complex as the one encoded by our model. We would like our sampler to explore as many modes as possible so that it can suggest multiple, high-probability coloring states.

To accelerate sampling, we use parallel tempering, a technique that runs multiple MCMC chains in parallel at different temperatures and swaps their states periodically~\cite{ParallelTempering}. Large values of $\tau$ yield flatter probability landscapes, so these `hot' chains are more likely to take large jumps across the state space. `Cool' chains, on the other hand, reject almost all proposed states that do not lead to higher probabilities, thus behaving like local hill-climbing optimizers. Running multiple chains in parallel allows the total system to alternatively explore and refine different coloring configurations.

Finally, we use maximimum marginal relevance (MMR) to enforce diversity in the set of suggestions returned by the sampler~\cite{MMR}. MMR is a technique from information retrieval that re-ranks every item in a list according to a linear combination of relevance (model score, in our case) and similarity to the items preceding it. The similarity metric we use for two colorings of a pattern $\mathcal{P}$ is
%% Pattern similarity metric
\begin{equation*}
\text{sim}(\mathbf{c_1}, \mathbf{c_2}, \mathcal{P}) = - \sum_{g \in P} {A_g \cdot ||\mathbf{c_1}(g) - \mathbf{c_2}(g)||}
\end{equation*}
%%
which is the area-weighted sum of \lab distances between the corresponding colors in each coloring~\remark{Notation, terminology subject to change}.

\subsection{Weight Learning}
\label{sec:weights}

\newcommand{\pattern}{\mathcal{P}}
\newcommand{\model}{\mathcal{M}}
\newcommand{\template}{\mathcal{T}}
\newcommand{\weights}{\mathbf{w}}
\newcommand{\dataset}{\mathcal{D}}
\newcommand{\factor}{\mathcal{F}}
\newcommand{\variable}[1]{\mathbf{#1}}
\newcommand{\stats}[3]{\phi_{#1}(#2,#3)}
\newcommand{\expectation}{\mathds{E}}

\remark{Notation in here needs work + synchronization with previous notation.}

The model defined thus far has several terms $\template$, and each has a weight parameter $\weights_\template$. These weights control the relative importance of the different terms in the model---so how should they be set?

As a first approach, we could assume that all terms are equally important. Figure ? shows several high-scoring samples from the resulting model.~\remark{Insert figure, discuss problems: adjacent equi-luminant regions, etc.} These results suggest that some terms are, in fact, more critical than others.

A better approach is to set the weights so that the resulting model is highly likely to generate the training examples. That is, we would like to tune the weights so that the training examples---the patterns whose style we want to reproduce---have high probability under the model. This weight-tuning problem is an instance of maximum-likelihood parameter estimation~\cite{PGMBook}. The log-likelihood function of our model is
%% log-likelihood function
\begin{multline*}
\ell(\weights : \dataset) =
	\sum_{\pattern \in \dataset}
	(
		\sum_{\template \in \model}
			\sum_{\factor \in \template}
				\weights_\template \cdot \stats{\template}{\variable{c_\factor}(\pattern)}{\variable{f_\factor}(\pattern)} \\
		- \ln{Z_\pattern(\weights)}
	)
%%\ell(w : \mathcal{D}) = \sum_{T \in \mathcal{T}} { \sum_{F \in T} {w_T \phi(\mathbf{c_F}(\mathcal{P}), \mathbf{f_F}(\mathcal{P}))} }
\end{multline*}
%%
where $Z_\pattern(\weights)$ is the pattern-dependent partition function which normalizes the distribution. Convex log-likelihoods such as this one are typically maximized via gradient ascent. The partial derivatives of this function with respect to the weights are given by
%% Partial derivatives
\begin{multline*}
\frac{\partial}{\partial \weights_\template} \ell(\weights : \dataset) = 
	\sum_{\pattern \in \dataset}
		\sum_{\factor \in \template}
		(
			\stats{\template}{\variable{c_\factor}(\pattern)}{\variable{f_\factor}(\pattern)} \\
				- \expectation_\weights[\stats{\template}{\variable{C_\factor}}{\variable{f_\factor}(\pattern)}]
		)
\end{multline*}
%%
where $\expectation_\weights$ denotes an expectation under the model with weights $\weights$. Unfortunately, these quantities are extremely expensive to compute. The expectation term requires probabilistic inference---an NP-complete problem---for training pattern, for every iteration of gradient ascent.

This computational intractability has motivated the development of alternative, `biased' parameter estimation schemes which do not directly maximize the likelihood function but nevertheless yield parameters that give high likelihoods~\cite{NonMLEParameterEstimation}. We use one such method, called \emph{Contrastive Divergence} (CD)~\cite{ContrastiveDivergence}. CD uses the following approximation to the likelihood gradient:
%% CD gradient
\begin{multline*}
CD(\weights : \dataset) = 
	\sum_{\pattern \in \dataset}
		\sum_{\factor \in \template}
		(
			\stats{\template}{\variable{c_\factor}(\pattern)}{\variable{f_\factor}(\pattern)} \\
			- \stats{\template}{\variable{\hat{c}_\factor}}{\variable{f_\factor}(\pattern)}
		)
\end{multline*}
%%
where $\variable{\hat{c}_\factor}$ is the coloring state obtained by running an MCMC chain for one step from the initial state $\variable{c_\factor}(\pattern)$. CD essentially forms a local approximation to the likelihood gradient around the neighborhood of state $\variable{c_\factor}(\pattern)$. Since all terms in our model are conditional log-probabilities, we enforce the additional constraint during optimization that all weights be positive.~\remark{Happy to take this out if we can find a way around it}

While the exact weights learned depend on the training dataset, there are several strong general trends.~\remark{Discuss trends here: Color compat, perceptual difference strong. Name stuff is not strong. Etc.} The colorings shown in Figure~\ref{fig:teaser} were sampled from a model with automatically-tuned weights.

\subsection{Implementation}
\label{sec:implementation}

Our prototype implementation of this model is written in the Scala programming language, using the Factorie toolkit for probabilistic modeling~\cite{Factorie}. To evaluate the color compatibility term, it uses the reference MATLAB implementation provided by O'Donovan et al.~\shortcite{ODonovan}. A link to the source code can be found on the project website.